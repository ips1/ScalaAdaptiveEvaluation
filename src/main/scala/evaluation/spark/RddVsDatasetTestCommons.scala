package evaluation.spark

import java.io.PrintWriter

import org.apache.spark.sql.SparkSession
import evaluation.spark.apiadaptors.{DataHolder, DatasetData}
import evaluation.spark.data.{DataProvider, Record}
import evaluation.spark.queries.{GroupPairsByIdQuery, MultipleRecordsGroupingQuery, Query}

import scalaadaptive.api.Adaptive
import scalaadaptive.core.configuration.defaults.DefaultConfiguration

/**
  * Created by Petr Kubat on 7/18/17.
  *
  * A common class for the RDD vs. Dataset test.
  *
  * The environment should be set by changing the .master("local[1]")
  *
  * The specified combined query will be executed on data (represented by both RDD and Dataset) generated by a
  * dataProvider. The query and dataProvider are set by individual tests.
  *
  * The tests consists of testCount runs on data of size either args[0] or dataSize. The combined query analytics
  * data are printed out at the end of the test.
  *
  */
class RddVsDatasetTestCommons[TDataType] {
  val dataSize = 200000
  val testCount = 100

  def createSparkSession(): SparkSession =
    SparkSession
      .builder
      .master("local[1]")
      .appName("RddVsDatasetTest Test")
      .config("evaluation.spark.sql.adaptive.enabled", value = true)
      .getOrCreate()

  def runTest(data: DataHolder[TDataType], query: Query[TDataType], spark: SparkSession, iterationNo: Int): Unit = {
    val startTime = System.nanoTime
    val count = query.query(data)
    val endTime = System.nanoTime

    val duration = endTime - startTime
    println(s"Iteration $iterationNo: $duration ms")
  }

  def execute(args: Array[String], dataProvider: DataProvider[TDataType], queryProvider: (SparkSession) => Query[TDataType]): Unit = {
    val spark = createSparkSession()
    val size = if (args.length > 0) args(0).toInt else dataSize

    Adaptive.initialize(new DefaultConfiguration)
    val selector = new GroupPairsByIdQuery

    val data = dataProvider.generateData(size, spark)
    val query = queryProvider(spark)

    Seq.range(0, testCount).foreach(i => {
      runTest(data, query, spark, i)
    })

    spark.stop()

    import scalaadaptive.api.Implicits._
    query.query.getAnalyticsData.foreach(d => d.saveData(new PrintWriter(System.out)))
  }
}
